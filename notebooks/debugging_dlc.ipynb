{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import PIL.Image as Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\SLEAP_models\\test\\labels.v001.pkg.slp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='converted_project'\n",
    "scorer='me'\n",
    "base_output_dir=r\"D:\\SLEAP_models\\test\\converted_project\"\n",
    "\n",
    "\n",
    "config = {'Task': project_name,\n",
    "        'scorer': scorer,\n",
    "        'date': time.strftime(\"%Y-%m-%d\"),\n",
    "        'identity': None,\n",
    "        'project_path': base_output_dir,\n",
    "        'engine': 'pytorch',\n",
    "        'video_sets': {},\n",
    "        'start': 0,\n",
    "        'stop': 1,\n",
    "        'numframes2pick': 20,\n",
    "        'skeleton_color': 'black',\n",
    "        'pcutoff': 0.6,\n",
    "        'dotsize': 12,\n",
    "        'alphavalue': 0.6,\n",
    "        'colormap': 'rainbow',\n",
    "        'TrainingFraction': [0.95],\n",
    "        'iteration': 0,\n",
    "        'default_net_type': 'resnet_50',\n",
    "        'default_augmenter': 'default',\n",
    "        'snapshotindex': -1,\n",
    "        'detector_snapshotindex': -1,\n",
    "        'batch_size': 8,\n",
    "        'detector_batch_size': 1,\n",
    "        'cropping': False,\n",
    "        'x1': 0,\n",
    "        'x2': 640,\n",
    "        'y1': 277,\n",
    "        'y2': 624,\n",
    "        'corner2move2': [50, 50],\n",
    "        'move2corner': True,\n",
    "        'SuperAnimalConversionTables': None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(file_path, 'r') as hdf_file:\n",
    "    # Identify video names\n",
    "    video_names = {}\n",
    "    for video_group_name in hdf_file.keys():\n",
    "        if video_group_name.startswith('video'):\n",
    "            source_video_path = f'{video_group_name}/source_video'\n",
    "            if source_video_path in hdf_file:\n",
    "                source_video_json = hdf_file[source_video_path].attrs['json']\n",
    "                source_video_dict = json.loads(source_video_json)\n",
    "                video_filename = source_video_dict['backend']['filename']\n",
    "                video_names[video_group_name] = video_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keyiterator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvideo_names\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict_keyiterator' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_group, video_filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvideo_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m      2\u001b[0m     data_frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m     scorer_row, bodyparts_row, coords_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "for video_group, video_filename in video_names.items():\n",
    "    data_frames = []\n",
    "    scorer_row, bodyparts_row, coords_row = None, None, None\n",
    "    output_dir = os.path.join(\n",
    "        base_output_dir, \"labeled-data\", os.path.basename(video_filename).split('.')[0]\n",
    "    )\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # extract labeled frames and save them in a separate directory for each video\n",
    "    if video_group in hdf_file and 'video' in hdf_file[video_group]:\n",
    "        video_data = hdf_file[f'{video_group}/video'][:]\n",
    "        frame_numbers = hdf_file[f'{video_group}/frame_numbers'][:]\n",
    "        frame_names = []\n",
    "        for i, (img_bytes, frame_number) in enumerate(zip(video_data, frame_numbers)):\n",
    "            img = Image.open(io.BytesIO(np.array(img_bytes, dtype=np.uint8)))\n",
    "            img = np.array(img)\n",
    "            if i==0:\n",
    "                video_path = os.path.join(base_output_dir,'videos',video_names[video_group].split('/')[-1])\n",
    "                config['video_sets'][video_path] = {'crop': f'0, {img.shape[1]}, 0, {img.shape[0]}'}\n",
    "            frame_name = f\"img{str(frame_number).zfill(8)}.png\"\n",
    "            cv2.imwrite(f\"{output_dir}/{frame_name}\", img)\n",
    "            frame_names.append(frame_name)\n",
    "            print(f\"Saved frame {frame_number} as {frame_name}\")\n",
    "            \n",
    "            \n",
    "    # extract coordinates and save them in a separate directory for each video\n",
    "    if video_group in hdf_file and 'frames' in hdf_file:\n",
    "        frames_dataset = hdf_file['frames']\n",
    "        frame_references = {\n",
    "            frame['frame_id']: frame['frame_idx']\n",
    "            for frame in frames_dataset\n",
    "            if frame['video'] == int(video_group.replace('video', ''))\n",
    "        }\n",
    "\n",
    "\n",
    "        # Extract instances and points\n",
    "        points_dataset = hdf_file['points']\n",
    "        instances_dataset = hdf_file['instances']\n",
    "        \n",
    "        data = []\n",
    "        for frame_id in frame_references.keys():\n",
    "            for i in range(len(instances_dataset)):\n",
    "                if frame_id==instances_dataset[i]['frame_id']:\n",
    "                    point_id_start = instances_dataset[i+1]['point_id_start']\n",
    "                    point_id_end = instances_dataset[i+1]['point_id_end']\n",
    "                    break\n",
    "                \n",
    "            points = points_dataset[point_id_start:point_id_end]\n",
    "\n",
    "            keypoints_flat = []\n",
    "            for kp in points:\n",
    "                x, y, vis = kp['x'], kp['y'], kp['visible']\n",
    "                if np.isnan(x) or np.isnan(y) or vis==False:\n",
    "                    x, y = None, None\n",
    "                keypoints_flat.extend([x, y])\n",
    "\n",
    "            frame_idx = frame_references[frame_id]\n",
    "            data.append([frame_idx] + keypoints_flat)\n",
    "                \n",
    "            \n",
    "        # parse data\n",
    "        metadata_json = hdf_file['metadata'].attrs['json']\n",
    "        metadata_dict = json.loads(metadata_json)\n",
    "        nodes = metadata_dict['nodes']\n",
    "        links = metadata_dict['skeletons'][0]['links']\n",
    "        \n",
    "        keypoints = [node['name'] for node in nodes]\n",
    "        skeleton = [[keypoints[l['source']], keypoints[l['target']]] for l in links]\n",
    "        config['skeleton'] = skeleton\n",
    "        \n",
    "        keypoints_ids = [n['id'] for n in metadata_dict['skeletons'][0]['nodes']]\n",
    "        keypoints_ordered = [keypoints[idx] for idx in keypoints_ids]\n",
    "        config['bodyparts'] = keypoints_ordered\n",
    "\n",
    "        columns = [\n",
    "            'frame'\n",
    "        ] + [\n",
    "            f'{kp}_x' for kp in keypoints_ordered\n",
    "        ] + [\n",
    "            f'{kp}_y' for kp in keypoints_ordered\n",
    "        ]\n",
    "        scorer_row = ['scorer'] + [f'{scorer}'] * (len(columns) - 1)\n",
    "        bodyparts_row = ['bodyparts'] + [f'{kp}' for kp in keypoints_ordered for _ in (0, 1)]\n",
    "        coords_row = ['coords'] + ['x', 'y'] * len(keypoints_ordered)\n",
    "\n",
    "        labels_df = pd.DataFrame(data, columns=columns)\n",
    "        video_base_name = os.path.basename(video_filename).split('.')[0]\n",
    "        labels_df['frame'] = labels_df['frame'].apply(\n",
    "            lambda x: (\n",
    "                f\"labeled-data/{video_base_name}/\"\n",
    "                f\"img{str(int(x)).zfill(8)}.png\"\n",
    "            )\n",
    "        )\n",
    "        labels_df = labels_df.groupby('frame', as_index=False).first()\n",
    "        data_frames.append(labels_df)\n",
    "            \n",
    "        # Combine all data frames into a single DataFrame\n",
    "        combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "            \n",
    "        header_df = pd.DataFrame(\n",
    "            [scorer_row, bodyparts_row, coords_row],\n",
    "            columns=combined_df.columns\n",
    "        )\n",
    "        final_df = pd.concat([header_df, combined_df], ignore_index=True)\n",
    "        final_df.columns = [None] * len(final_df.columns)  # Set header to None\n",
    "        \n",
    "        # Save concatenated labels\n",
    "        final_df.to_csv(os.path.join(output_dir, f\"CollectedData_{scorer}.csv\"), index=False, header=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_pkg_slp(file_path, base_output_dir, project_name='converted_project', scorer='me'):\n",
    "\n",
    "    config = {'Task': project_name,\n",
    "              'scorer': scorer,\n",
    "              'date': time.strftime(\"%Y-%m-%d\"),\n",
    "              'identity': None,\n",
    "              'project_path': base_output_dir,\n",
    "              'engine': 'pytorch',\n",
    "              'video_sets': {},\n",
    "              'start': 0,\n",
    "              'stop': 1,\n",
    "              'numframes2pick': 20,\n",
    "              'skeleton_color': 'black',\n",
    "              'pcutoff': 0.6,\n",
    "              'dotsize': 12,\n",
    "              'alphavalue': 0.6,\n",
    "              'colormap': 'rainbow',\n",
    "              'TrainingFraction': [0.95],\n",
    "              'iteration': 0,\n",
    "              'default_net_type': 'resnet_50',\n",
    "              'default_augmenter': 'default',\n",
    "              'snapshotindex': -1,\n",
    "              'detector_snapshotindex': -1,\n",
    "              'batch_size': 8,\n",
    "              'detector_batch_size': 1,\n",
    "              'cropping': False,\n",
    "              'x1': 0,\n",
    "              'x2': 640,\n",
    "              'y1': 277,\n",
    "              'y2': 624,\n",
    "              'corner2move2': [50, 50],\n",
    "              'move2corner': True,\n",
    "              'SuperAnimalConversionTables': None\n",
    "              }\n",
    "    \n",
    "    \n",
    "    #create directories\n",
    "    if not os.path.exists(base_output_dir):\n",
    "        os.makedirs(base_output_dir)\n",
    "        os.makedirs(os.path.join(base_output_dir,\"labeled-data\"))\n",
    "        os.makedirs(os.path.join(base_output_dir,\"videos\"))\n",
    "        \n",
    "    #parse .slp file\n",
    "    with h5py.File(file_path, 'r') as hdf_file:\n",
    "        # Identify video names\n",
    "        video_names = {}\n",
    "        for video_group_name in hdf_file.keys():\n",
    "            if video_group_name.startswith('video'):\n",
    "                source_video_path = f'{video_group_name}/source_video'\n",
    "                if source_video_path in hdf_file:\n",
    "                    source_video_json = hdf_file[source_video_path].attrs['json']\n",
    "                    source_video_dict = json.loads(source_video_json)\n",
    "                    video_filename = source_video_dict['backend']['filename']\n",
    "                    video_names[video_group_name] = video_filename\n",
    "                    \n",
    "        # Extract and save images for each video\n",
    "        for video_group, video_filename in video_names.items():\n",
    "            data_frames = []\n",
    "            scorer_row, bodyparts_row, coords_row = None, None, None\n",
    "            output_dir = os.path.join(\n",
    "                base_output_dir, \"labeled-data\", os.path.basename(video_filename).split('.')[0]\n",
    "            )\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "            # extract labeled frames and save them in a separate directory for each video\n",
    "            if video_group in hdf_file and 'video' in hdf_file[video_group]:\n",
    "                video_data = hdf_file[f'{video_group}/video'][:]\n",
    "                frame_numbers = hdf_file[f'{video_group}/frame_numbers'][:]\n",
    "                frame_names = []\n",
    "                for i, (img_bytes, frame_number) in enumerate(zip(video_data, frame_numbers)):\n",
    "                    img = Image.open(io.BytesIO(np.array(img_bytes, dtype=np.uint8)))\n",
    "                    img = np.array(img)\n",
    "                    if i==0:\n",
    "                        video_path = os.path.join(base_output_dir,'videos',video_names[video_group].split('/')[-1])\n",
    "                        config['video_sets'][video_path] = {'crop': f'0, {img.shape[1]}, 0, {img.shape[0]}'}\n",
    "                    frame_name = f\"img{str(frame_number).zfill(8)}.png\"\n",
    "                    cv2.imwrite(f\"{output_dir}/{frame_name}\", img)\n",
    "                    frame_names.append(frame_name)\n",
    "                    print(f\"Saved frame {frame_number} as {frame_name}\")\n",
    "                    \n",
    "                    \n",
    "            # extract coordinates and save them in a separate directory for each video\n",
    "            if video_group in hdf_file and 'frames' in hdf_file:\n",
    "                frames_dataset = hdf_file['frames']\n",
    "                frame_references = {\n",
    "                    frame['frame_id']: frame['frame_idx']\n",
    "                    for frame in frames_dataset\n",
    "                    if frame['video'] == int(video_group.replace('video', ''))\n",
    "                }\n",
    "\n",
    "\n",
    "                # Extract instances and points\n",
    "                points_dataset = hdf_file['points']\n",
    "                instances_dataset = hdf_file['instances']\n",
    "                \n",
    "                data = []\n",
    "                for frame_id in frame_references.keys():\n",
    "                    for i in range(len(instances_dataset)):\n",
    "                        if frame_id==instances_dataset[i]['frame_id']:\n",
    "                            point_id_start = instances_dataset[i+1]['point_id_start']\n",
    "                            point_id_end = instances_dataset[i+1]['point_id_end']\n",
    "                            break\n",
    "                        \n",
    "                    points = points_dataset[point_id_start:point_id_end]\n",
    "\n",
    "                    keypoints_flat = []\n",
    "                    for kp in points:\n",
    "                        x, y, vis = kp['x'], kp['y'], kp['visible']\n",
    "                        if np.isnan(x) or np.isnan(y) or vis==False:\n",
    "                            x, y = None, None\n",
    "                        keypoints_flat.extend([x, y])\n",
    "\n",
    "                    frame_idx = frame_references[frame_id]\n",
    "                    data.append([frame_idx] + keypoints_flat)\n",
    "                        \n",
    "                    \n",
    "                # parse data\n",
    "                metadata_json = hdf_file['metadata'].attrs['json']\n",
    "                metadata_dict = json.loads(metadata_json)\n",
    "                nodes = metadata_dict['nodes']\n",
    "                links = metadata_dict['skeletons'][0]['links']\n",
    "                \n",
    "                keypoints = [node['name'] for node in nodes]\n",
    "                skeleton = [[keypoints[l['source']], keypoints[l['target']]] for l in links]\n",
    "                config['skeleton'] = skeleton\n",
    "                \n",
    "                keypoints_ids = [n['id'] for n in metadata_dict['skeletons'][0]['nodes']]\n",
    "                keypoints_ordered = [keypoints[idx] for idx in keypoints_ids]\n",
    "                config['bodyparts'] = keypoints_ordered\n",
    "\n",
    "                columns = [\n",
    "                    'frame'\n",
    "                ] + [\n",
    "                    f'{kp}_x' for kp in keypoints_ordered\n",
    "                ] + [\n",
    "                    f'{kp}_y' for kp in keypoints_ordered\n",
    "                ]\n",
    "                scorer_row = ['scorer'] + [f'{scorer}'] * (len(columns) - 1)\n",
    "                bodyparts_row = ['bodyparts'] + [f'{kp}' for kp in keypoints_ordered for _ in (0, 1)]\n",
    "                coords_row = ['coords'] + ['x', 'y'] * len(keypoints_ordered)\n",
    "\n",
    "                labels_df = pd.DataFrame(data, columns=columns)\n",
    "                video_base_name = os.path.basename(video_filename).split('.')[0]\n",
    "                labels_df['frame'] = labels_df['frame'].apply(\n",
    "                    lambda x: (\n",
    "                        f\"labeled-data/{video_base_name}/\"\n",
    "                        f\"img{str(int(x)).zfill(8)}.png\"\n",
    "                    )\n",
    "                )\n",
    "                labels_df = labels_df.groupby('frame', as_index=False).first()\n",
    "                data_frames.append(labels_df)\n",
    "                    \n",
    "                # Combine all data frames into a single DataFrame\n",
    "                combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "                    \n",
    "                header_df = pd.DataFrame(\n",
    "                    [scorer_row, bodyparts_row, coords_row],\n",
    "                    columns=combined_df.columns\n",
    "                )\n",
    "                final_df = pd.concat([header_df, combined_df], ignore_index=True)\n",
    "                final_df.columns = [None] * len(final_df.columns)  # Set header to None\n",
    "                \n",
    "                # Save concatenated labels\n",
    "                final_df.to_csv(os.path.join(output_dir, f\"CollectedData_{scorer}.csv\"), index=False, header=None)\n",
    "                \n",
    "    with open(os.path.join(base_output_dir, 'config.yaml'), 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False) \n",
    "    \n",
    "\n",
    "\n",
    "# Extract and save labeled data from SLEAP project\n",
    "extract_frames_from_pkg_slp(slp_file, dlc_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
